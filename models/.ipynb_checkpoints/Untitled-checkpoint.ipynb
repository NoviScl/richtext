{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from data_reader import *\n",
    "from evaluate_new import *\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics \n",
    "from keras_contrib.layers import CRF\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"../../data/test_docs/\"\n",
    "def get_doc_test(gold, text):\n",
    "    ## gold: gold data\n",
    "    ## text: full text file\n",
    "    test_labels = []\n",
    "    test_doc = []\n",
    "    with open(doc_dir+gold, 'r') as doc_labels, open(doc_dir+text, 'r') as doc_text:\n",
    "        d_labels = doc_labels.readlines()\n",
    "        d_text = doc_text.readlines()\n",
    "        assert len(d_labels) == len(d_text), \"Mismatch\"\n",
    "        for i in range(len(d_labels)):\n",
    "            ## label: start_id end_id data_id pub_id\n",
    "            test_labels.append(d_labels[i].strip())\n",
    "            \n",
    "            text = d_text[i].strip()\n",
    "            text = re.sub('\\d', '0', text)\n",
    "            text = re.sub('[^ ]- ', '', text)\n",
    "            \n",
    "            test_doc.append(text)\n",
    "    return test_labels, test_doc\n",
    "\n",
    "def read_doc(doc, labels):\n",
    "    doc = doc.strip().split()\n",
    "    labels = labels.strip().split('|')\n",
    "    labels = [la.split() for la in labels]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            labels[i][j] = int(labels[i][j])\n",
    "\n",
    "    res_labels = [0]*len(doc)\n",
    "    for la in labels:\n",
    "        if la[2]!=0:\n",
    "            start = la[0]\n",
    "            end = la[1]\n",
    "            res_labels[start : end+1] = [1]*(end+1-start)\n",
    "    return [(doc[i], str(res_labels[i])) for i in range(len(doc))]\n",
    "\n",
    "# predict one doc\n",
    "def doc_pred(model, doc, MAXLEN):\n",
    "    splits = []\n",
    "    for i in range(0, len(doc), MAXLEN):\n",
    "        splits.append(doc[i : i+MAXLEN])\n",
    "    splits = tokenizer.texts_to_sequences(splits)\n",
    "    splits = pad_sequences(splits, maxlen=MAXLEN)\n",
    "    preds = model.predict(splits)\n",
    "    \n",
    "    preds = [1 if p>=0.5 else 0 for pd in preds for p in pd]\n",
    "    return preds\n",
    "\n",
    "def doc_eval(model, doc_test, doc_out_dir, gold_dir, MAXLEN=30):\n",
    "    '''\n",
    "    model: trained model \n",
    "    doc_test: processed doc test input\n",
    "    doc_out_dir: dir to store predicted results\n",
    "    gold_dir: gold data dir, for evaluation\n",
    "    prediction format: start_id end_id\n",
    "    '''\n",
    "    doc_preds = [doc_pred(model, d, MAXLEN) for d in doc_test]\n",
    "    doc_preds = [[int(a) for a in x] for x in doc_preds]\n",
    "    with open(doc_out_dir, 'w') as fout:\n",
    "        for i in range(len(doc_preds)):\n",
    "            first = 0\n",
    "            j = 0\n",
    "            string = ''\n",
    "            no_mention = True\n",
    "            while j<len(doc_preds[i]):\n",
    "                while j<len(doc_preds[i]) and doc_preds[i][j]== 0:\n",
    "                    j+=1\n",
    "                if j<len(doc_preds[i]) and doc_preds[i][j] == 1:\n",
    "                    no_mention=False\n",
    "                    start = j\n",
    "                    while j+1<len(doc_preds[i]) and doc_preds[i][j+1]==1:\n",
    "                        j+=1\n",
    "                    end = j \n",
    "                    if first > 0:\n",
    "                        string += \" | \"\n",
    "                    string += (str(start)+' '+str(end))\n",
    "                    j+=1\n",
    "                    first += 1\n",
    "            if no_mention:\n",
    "                fout.write(\"-1 -1\"'\\n')\n",
    "            else:\n",
    "                fout.write(string+'\\n')\n",
    "    print ('evaluating data from: ', doc_out_dir)\n",
    "    print ('doc exact: ', doc_exact_match(doc_out_dir, gold_dir))\n",
    "    print ('doc partial: ', doc_partial_match(doc_out_dir, gold_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [int(label) for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##load glove\n",
    "embedding_index = {}\n",
    "f = open('../../glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "vocab_size = 100000\n",
    "maxlen = 30\n",
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dir, val_dir):\n",
    "    train_sents = get_sents(train_dir)\n",
    "    val_sents = get_sents(val_dir)\n",
    "    \n",
    "    X_train = [sent2tokens(s) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "    X_val = [sent2tokens(s) for s in val_sents]\n",
    "    Y_val = [sent2labels(s) for s in val_sents]\n",
    "    \n",
    "    tokenizer = Tokenizer(vocab_size)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    word_index = tokenizer.word_index\n",
    "    print ('Total vocab found: ', len(word_index))\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    counter = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            break\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            counter += 1\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.randn(emb_dim)\n",
    "    print (\"{}/{} words covered in glove\".format(counter, vocab_size))\n",
    "    \n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_val = pad_sequences(X_val, maxlen=maxlen)\n",
    "        \n",
    "    Y_train = np.asarray(Y_train)\n",
    "    Y_val = np.asarray(Y_val)\n",
    "    \n",
    "    Y_train = np.expand_dims(Y_train, axis=2)\n",
    "    Y_val = np.expand_dims(Y_val, axis=2)\n",
    "    \n",
    "    ##build model\n",
    "    input = Input(shape=(maxlen,))\n",
    "    model = Embedding(vocab_size, emb_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(100, return_sequences=True))(model)\n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(model)\n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    earlyStop = [EarlyStopping(monitor='val_loss', patience=1)]\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_data=(X_val, Y_val), \n",
    "        callbacks=earlyStop) \n",
    "    \n",
    "    Y_pred = model.predict(X_val)\n",
    "    print (f1_score(Y_val, Y_pred))\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test_y, doc_test_x = get_doc_test('test_doc_gold', 'test_docs')\n",
    "doc_tests = [read_doc(doc_test_x[d], doc_test_y[d]) for d in range(len(doc_test_x))]\n",
    "doc_tests = [sent2tokens(s) for s in doc_tests]\n",
    "\n",
    "zero_shot_y, zero_shot_x = get_doc_test('zero_shot_doc_gold', 'zero_shot_docs')\n",
    "zero_shot_tests = [read_doc(zero_shot_x[d], zero_shot_y[d]) for d in range(len(zero_shot_x))]\n",
    "zero_shot_tests = [sent2tokens(s) for s in zero_shot_tests]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab found:  110789\n",
      "48703/100000 words covered in glove\n",
      "Train on 343534 samples, validate on 38171 samples\n",
      "Epoch 1/10\n",
      " 92096/343534 [=======>......................] - ETA: 10:32 - loss: 0.0296 - acc: 0.9900"
     ]
    }
   ],
   "source": [
    "for i in [300]:\n",
    "    DIR = '../../data/data_30_'+str(i)+'neg/'\n",
    "    out = '../../BiLSTM_outputs/'\n",
    "    if not os.path.exists(out):\n",
    "        os.makedirs(out)\n",
    "    model, history = run(DIR+'train.txt', DIR+'validate.txt')\n",
    "    doc_eval(model, doc_tests, out+'doc_30_'+str(i)+'neg', '../../data/test_docs/test_doc_gold')\n",
    "    doc_eval(model, zero_shot_tests, out+'zeroshot_30_'+str(i)+'neg', '../../data/test_docs/zero_shot_doc_gold')\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
