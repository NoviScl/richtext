{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from data_reader import *\n",
    "from evaluate_new import *\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics \n",
    "from keras_contrib.layers import CRF\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"../../data/test_docs/\"\n",
    "def get_doc_test(gold, text):\n",
    "    ## gold: gold data\n",
    "    ## text: full text file\n",
    "    test_labels = []\n",
    "    test_doc = []\n",
    "    with open(doc_dir+gold, 'r') as doc_labels, open(doc_dir+text, 'r') as doc_text:\n",
    "        d_labels = doc_labels.readlines()\n",
    "        d_text = doc_text.readlines()\n",
    "        assert len(d_labels) == len(d_text), \"Mismatch\"\n",
    "        for i in range(len(d_labels)):\n",
    "            ## label: start_id end_id data_id pub_id\n",
    "            test_labels.append(d_labels[i].strip())\n",
    "            \n",
    "            text = d_text[i].strip()\n",
    "            text = re.sub('\\d', '0', text)\n",
    "            text = re.sub('[^ ]- ', '', text)\n",
    "            \n",
    "            test_doc.append(text)\n",
    "    return test_labels, test_doc\n",
    "\n",
    "def read_doc(doc, labels):\n",
    "    doc = doc.strip().split()\n",
    "    labels = labels.strip().split('|')\n",
    "    labels = [la.split() for la in labels]\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            labels[i][j] = int(labels[i][j])\n",
    "\n",
    "    res_labels = [0]*len(doc)\n",
    "    for la in labels:\n",
    "        if la[2]!=0:\n",
    "            start = la[0]\n",
    "            end = la[1]\n",
    "            res_labels[start : end+1] = [1]*(end+1-start)\n",
    "    return [(doc[i], str(res_labels[i])) for i in range(len(doc))]\n",
    "\n",
    "# predict one doc\n",
    "def doc_pred(model, doc, MAXLEN):\n",
    "    splits = []\n",
    "    for i in range(0, len(doc), MAXLEN):\n",
    "        splits.append(doc[i : i+MAXLEN])\n",
    "    splits = tokenizer.texts_to_sequences(splits)\n",
    "    splits = pad_sequences(splits, maxlen=MAXLEN)\n",
    "    preds = model.predict(splits)\n",
    "    \n",
    "    preds = [1 if p>=0.5 else 0 for pd in preds for p in pd]\n",
    "    return preds\n",
    "\n",
    "def doc_eval(model, doc_test, doc_out_dir, gold_dir, MAXLEN=30):\n",
    "    '''\n",
    "    model: trained model \n",
    "    doc_test: processed doc test input\n",
    "    doc_out_dir: dir to store predicted results\n",
    "    gold_dir: gold data dir, for evaluation\n",
    "    prediction format: start_id end_id\n",
    "    '''\n",
    "    doc_preds = [doc_pred(model, d, MAXLEN) for d in doc_test]\n",
    "    doc_preds = [[int(a) for a in x] for x in doc_preds]\n",
    "    with open(doc_out_dir, 'w') as fout:\n",
    "        for i in range(len(doc_preds)):\n",
    "            first = 0\n",
    "            j = 0\n",
    "            string = ''\n",
    "            no_mention = True\n",
    "            while j<len(doc_preds[i]):\n",
    "                while j<len(doc_preds[i]) and doc_preds[i][j]== 0:\n",
    "                    j+=1\n",
    "                if j<len(doc_preds[i]) and doc_preds[i][j] == 1:\n",
    "                    no_mention=False\n",
    "                    start = j\n",
    "                    while j+1<len(doc_preds[i]) and doc_preds[i][j+1]==1:\n",
    "                        j+=1\n",
    "                    end = j \n",
    "                    if first > 0:\n",
    "                        string += \" | \"\n",
    "                    string += (str(start)+' '+str(end))\n",
    "                    j+=1\n",
    "                    first += 1\n",
    "            if no_mention:\n",
    "                fout.write(\"-1 -1\"'\\n')\n",
    "            else:\n",
    "                fout.write(string+'\\n')\n",
    "    print ('evaluating data from: ', doc_out_dir)\n",
    "    print ('doc exact: ', doc_exact_match(doc_out_dir, gold_dir))\n",
    "    print ('doc partial: ', doc_partial_match(doc_out_dir, gold_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [int(label) for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##load glove\n",
    "embedding_index = {}\n",
    "f = open('../../glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "vocab_size = 100000\n",
    "maxlen = 30\n",
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_dir, val_dir):\n",
    "    train_sents = get_sents(train_dir)\n",
    "    val_sents = get_sents(val_dir)\n",
    "    \n",
    "    X_train = [sent2tokens(s) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "    X_val = [sent2tokens(s) for s in val_sents]\n",
    "    Y_val = [sent2labels(s) for s in val_sents]\n",
    "    \n",
    "    tokenizer = Tokenizer(vocab_size)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    word_index = tokenizer.word_index\n",
    "    print ('Total vocab found: ', len(word_index))\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    counter = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            break\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            counter += 1\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.randn(emb_dim)\n",
    "    print (\"{}/{} words covered in glove\".format(counter, vocab_size))\n",
    "    \n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_val = pad_sequences(X_val, maxlen=maxlen)\n",
    "        \n",
    "    Y_train = np.asarray(Y_train)\n",
    "    Y_val = np.asarray(Y_val)\n",
    "    \n",
    "    Y_train = np.expand_dims(Y_train, axis=2)\n",
    "    Y_val = np.expand_dims(Y_val, axis=2)\n",
    "    \n",
    "    ##build model\n",
    "    input = Input(shape=(maxlen,))\n",
    "    model = Embedding(vocab_size, emb_dim, weights=[embedding_matrix], input_length=maxlen, trainable=False)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(100, return_sequences=True))(model)\n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(model)\n",
    "\n",
    "    model = Model(input, out)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    earlyStop = [EarlyStopping(monitor='val_loss', patience=1)]\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_data=(X_val, Y_val), \n",
    "        callbacks=earlyStop) \n",
    "    \n",
    "    Y_pred = model.predict(X_val)\n",
    "    print (f1_score(Y_val, Y_pred))\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test_y, doc_test_x = get_doc_test('test_doc_gold', 'test_docs')\n",
    "doc_tests = [read_doc(doc_test_x[d], doc_test_y[d]) for d in range(len(doc_test_x))]\n",
    "doc_tests = [sent2tokens(s) for s in doc_tests]\n",
    "\n",
    "zero_shot_y, zero_shot_x = get_doc_test('zero_shot_doc_gold', 'zero_shot_docs')\n",
    "zero_shot_tests = [read_doc(zero_shot_x[d], zero_shot_y[d]) for d in range(len(zero_shot_x))]\n",
    "zero_shot_tests = [sent2tokens(s) for s in zero_shot_tests]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab found:  110789\n",
      "48703/100000 words covered in glove\n",
      "Train on 343534 samples, validate on 38171 samples\n",
      "Epoch 1/10\n",
      " 94016/343534 [=======>......................] - ETA: 10:25 - loss: 0.0294 - acc: 0.9900"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8c701836fc3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'validate.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdoc_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_tests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'doc_30_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../data/test_docs/test_doc_gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdoc_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_shot_tests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'zeroshot_30_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../data/test_docs/zero_shot_doc_gold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-5e99a18f5616>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(train_dir, val_dir)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mearlyStop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     history = model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_data=(X_val, Y_val), \n\u001b[0;32m---> 52\u001b[0;31m         callbacks=earlyStop) \n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/salt/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in [300]:\n",
    "    DIR = '../../data/data_30_'+str(i)+'neg/'\n",
    "    out = '../../BiLSTM_outputs/'\n",
    "    if not os.path.exists(out):\n",
    "        os.makedirs(out)\n",
    "    model, history = run(DIR+'train.txt', DIR+'validate.txt')\n",
    "    doc_eval(model, doc_tests, out+'doc_30_'+str(i)+'neg', '../../data/test_docs/test_doc_gold')\n",
    "    doc_eval(model, zero_shot_tests, out+'zeroshot_30_'+str(i)+'neg', '../../data/test_docs/zero_shot_doc_gold')\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
